# AI/ML Integration Package

Complete AI/ML infrastructure for machine learning model serving, feature engineering, and inference pipelines.

## Features

### ğŸ¤– Model Management
- Multi-provider support (OpenAI, HuggingFace, local models)
- Model versioning and lifecycle management
- Automatic model loading and unloading
- Model metrics and monitoring

### ğŸ”„ Inference Pipeline
- Multi-step ML pipelines
- Pre/post-processing transforms
- Batch processing support
- Pipeline chaining

### ğŸ’¾ Feature Store
- Feature storage and versioning
- Feature groups and vectors
- Real-time feature serving
- Feature caching with TTL

### âš¡ Performance
- Inference result caching
- Batch inference support
- Async prediction
- Connection pooling

### ğŸ“Š Monitoring
- Request metrics (latency, throughput)
- Error tracking
- Cache hit rates
- Model usage statistics

## Quick Start

### 1. Basic Model Inference

```go
package main

import (
    "context"
    "log"
    
    "neonexcore/pkg/ai"
)

func main() {
    // Create model manager
    manager := ai.NewModelManager()

    // Register OpenAI provider
    openai := ai.NewOpenAIProvider(&ai.OpenAIConfig{
        APIKey: "your-api-key",
    })
    manager.RegisterProvider("openai", openai)

    // Load model
    model, err := manager.LoadModel(&ai.ModelConfig{
        ID:       "gpt-4",
        Name:     "GPT-4",
        Type:     ai.ModelTypeTextGeneration,
        Provider: "openai",
    })
    if err != nil {
        log.Fatal(err)
    }

    // Perform inference
    ctx := context.Background()
    output, err := manager.Predict(ctx, &ai.InferenceInput{
        ModelID: "gpt-4",
        Data:    "Explain quantum computing in simple terms",
        Parameters: map[string]interface{}{
            "type":        "chat",
            "temperature": 0.7,
            "max_tokens":  500,
        },
    })
    
    if err != nil {
        log.Fatal(err)
    }

    log.Printf("Result: %+v", output.Result)
    log.Printf("Latency: %v", output.Latency)
}
```

### 2. Feature Store

```go
// Create feature store
db, _ := gorm.Open(postgres.Open(dsn), &gorm.Config{})
featureStore := ai.NewFeatureStore(db)

// Set features
ctx := context.Background()
feature := &ai.Feature{
    Name:       "user_activity_score",
    EntityType: "user",
    EntityID:   "user-123",
    Values: map[string]interface{}{
        "login_count":     42,
        "purchase_count":  7,
        "avg_session_time": 1200,
    },
    Version: 1,
}

err := featureStore.SetFeature(ctx, feature)

// Get feature vector
vector, err := featureStore.GetFeatureVector(ctx, "user", "user-123", []string{
    "user_activity_score",
    "user_preferences",
})

// Use in model prediction
output, err := manager.Predict(ctx, &ai.InferenceInput{
    ModelID: "recommendation-model",
    Data:    vector,
})
```

### 3. ML Pipeline

```go
// Create pipeline manager
pipelineManager := ai.NewPipelineManager(manager)

// Define pipeline
pipeline := &ai.Pipeline{
    ID:          "sentiment-pipeline",
    Name:        "Sentiment Analysis Pipeline",
    Description: "Text preprocessing â†’ Sentiment model â†’ Post-processing",
    Steps: []ai.PipelineStep{
        {
            Name: "preprocess",
            Type: ai.StepTypePreprocess,
            Transform: ai.TextPreprocessor,
        },
        {
            Name:    "sentiment-model",
            Type:    ai.StepTypeModel,
            ModelID: "sentiment-analyzer",
            Parameters: map[string]interface{}{
                "type": "classification",
            },
        },
        {
            Name: "extract-label",
            Type: ai.StepTypePostprocess,
            Transform: ai.JSONExtractor("label"),
        },
    },
}

err := pipelineManager.CreatePipeline(pipeline)

// Execute pipeline
result, err := pipelineManager.Execute(ctx, "sentiment-pipeline", "This product is amazing!")

log.Printf("Sentiment: %v", result.Output)
log.Printf("Pipeline latency: %v", result.Latency)

// Inspect step results
for _, stepResult := range result.StepResults {
    log.Printf("Step %s: %v (took %v)", 
        stepResult.StepName, 
        stepResult.Output, 
        stepResult.Latency,
    )
}
```

### 4. Feature Groups

```go
// Create feature group
group := &ai.FeatureGroup{
    Name:        "user-profile",
    Description: "User profile features for recommendations",
    EntityType:  "user",
    Features: []string{
        "age",
        "location",
        "interests",
        "activity_score",
        "purchase_history",
    },
    Version: 1,
}

err := featureStore.CreateFeatureGroup(ctx, group)

// Get all features in group
vector, err := featureStore.GetFeatureGroupVector(ctx, "user-profile", "user", "user-123")

// Use for batch inference
users := []string{"user-123", "user-456", "user-789"}
for _, userID := range users {
    vector, _ := featureStore.GetFeatureGroupVector(ctx, "user-profile", "user", userID)
    output, _ := manager.Predict(ctx, &ai.InferenceInput{
        ModelID: "recommendation-model",
        Data:    vector,
    })
    log.Printf("Recommendations for %s: %v", userID, output.Result)
}
```

### 5. Batch Processing Pipeline

```go
// Create batch processing pipeline
pipeline := &ai.Pipeline{
    ID:   "batch-classification",
    Name: "Batch Text Classification",
    Steps: []ai.PipelineStep{
        {
            Name: "batch-process",
            Type: ai.StepTypeTransform,
            Transform: ai.BatchProcessor(10, func(ctx context.Context, item interface{}) (interface{}, error) {
                // Process each item
                output, err := manager.Predict(ctx, &ai.InferenceInput{
                    ModelID: "classifier",
                    Data:    item,
                })
                return output.Result, err
            }),
        },
    },
}

pipelineManager.CreatePipeline(pipeline)

// Process batch
texts := []interface{}{
    "Text 1",
    "Text 2",
    "Text 3",
    // ... more texts
}

result, err := pipelineManager.Execute(ctx, "batch-classification", texts)
```

### 6. Caching & Performance

```go
// Inference results are automatically cached
// First call - hits the model
output1, _ := manager.Predict(ctx, &ai.InferenceInput{
    ModelID: "gpt-4",
    Data:    "Hello world",
})

// Second call - returns cached result (much faster)
output2, _ := manager.Predict(ctx, &ai.InferenceInput{
    ModelID: "gpt-4",
    Data:    "Hello world",
})

// Check cache stats
cache := manager.cache // Internal cache
stats := cache.GetStats()
log.Printf("Cache hit rate: %.2f%%", stats["hit_rate"].(float64) * 100)
```

## Architecture

### Model Manager

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Model Manager                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Model Registry               â”‚  â”‚
â”‚  â”‚  - GPT-4 (OpenAI)             â”‚  â”‚
â”‚  â”‚  - BERT (HuggingFace)         â”‚  â”‚
â”‚  â”‚  - Custom (Local)             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Inference Cache              â”‚  â”‚
â”‚  â”‚  - LRU eviction               â”‚  â”‚
â”‚  â”‚  - TTL expiration             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Provider Abstraction         â”‚  â”‚
â”‚  â”‚  - OpenAI API                 â”‚  â”‚
â”‚  â”‚  - HuggingFace API            â”‚  â”‚
â”‚  â”‚  - Local Models               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Feature Store

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Feature Store                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Feature Repository           â”‚  â”‚
â”‚  â”‚  - PostgreSQL/MySQL           â”‚  â”‚
â”‚  â”‚  - JSONB storage              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Feature Cache                â”‚  â”‚
â”‚  â”‚  - In-memory cache            â”‚  â”‚
â”‚  â”‚  - 5-minute TTL               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Feature Groups               â”‚  â”‚
â”‚  â”‚  - Group definitions          â”‚  â”‚
â”‚  â”‚  - Version management         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ML Pipeline

```
Input
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Preprocessing   â”‚
â”‚ - Text cleaning â”‚
â”‚ - Tokenization  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feature Extract â”‚
â”‚ - Get features  â”‚
â”‚ - Build vector  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model Inference â”‚
â”‚ - Load model    â”‚
â”‚ - Predict       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Postprocessing  â”‚
â”‚ - Format output â”‚
â”‚ - Add metadata  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
      Output
```

## Model Providers

### OpenAI Provider

```go
// Configure OpenAI
openai := ai.NewOpenAIProvider(&ai.OpenAIConfig{
    APIKey:  "sk-...",
    BaseURL: "https://api.openai.com/v1", // Optional
})

// Chat completion
output, _ := manager.Predict(ctx, &ai.InferenceInput{
    ModelID: "gpt-4",
    Data:    "Write a poem",
    Parameters: map[string]interface{}{
        "type":        "chat",
        "system":      "You are a poet",
        "temperature": 0.8,
        "max_tokens":  200,
    },
})

// Text embedding
output, _ := manager.Predict(ctx, &ai.InferenceInput{
    ModelID: "text-embedding-ada-002",
    Data:    "Text to embed",
    Parameters: map[string]interface{}{
        "type": "embedding",
    },
})
```

### Custom Provider (Extend)

```go
type CustomProvider struct {
    // Your implementation
}

func (p *CustomProvider) LoadModel(config *ai.ModelConfig) (*ai.Model, error) {
    // Load your model
}

func (p *CustomProvider) Predict(ctx context.Context, modelID string, input *ai.InferenceInput) (*ai.InferenceOutput, error) {
    // Perform inference
}

// Register
manager.RegisterProvider("custom", &CustomProvider{})
```

## Use Cases

### 1. **Content Moderation**
```go
pipeline := &ai.Pipeline{
    Steps: []ai.PipelineStep{
        {Name: "toxicity-check", ModelID: "toxicity-model"},
        {Name: "extract-score", Transform: ai.JSONExtractor("score")},
    },
}
```

### 2. **Recommendation System**
```go
// Get user features
vector, _ := featureStore.GetFeatureGroupVector(ctx, "user-profile", "user", userID)

// Get recommendations
output, _ := manager.Predict(ctx, &ai.InferenceInput{
    ModelID: "recommendation-model",
    Data:    vector,
})
```

### 3. **Sentiment Analysis**
```go
output, _ := manager.Predict(ctx, &ai.InferenceInput{
    ModelID: "sentiment-model",
    Data:    "Customer feedback text",
})
```

### 4. **Named Entity Recognition**
```go
output, _ := manager.Predict(ctx, &ai.InferenceInput{
    ModelID: "ner-model",
    Data:    "John works at Microsoft in Seattle",
})
```

## Best Practices

### 1. **Feature Engineering**
- Compute features offline when possible
- Cache frequently accessed features
- Version features for reproducibility
- Use feature groups for organization

### 2. **Model Serving**
- Load models on startup
- Use caching for repeated queries
- Set appropriate timeouts
- Monitor model performance

### 3. **Pipeline Design**
- Keep steps independent
- Use batch processing for efficiency
- Add error handling at each step
- Log step results for debugging

### 4. **Performance**
- Enable caching (1-hour TTL recommended)
- Use batch inference for multiple items
- Warm up models on startup
- Monitor latency and optimize

### 5. **Monitoring**
- Track model metrics (latency, errors)
- Monitor cache hit rates
- Set up alerts for high error rates
- Log all predictions for auditing

## Performance

- **Inference Latency**: 50-500ms (model dependent)
- **Cache Hit Rate**: 60-80% for common queries
- **Feature Store**: <10ms for cached features
- **Pipeline Overhead**: <5ms per step
- **Batch Processing**: 10-100x faster than individual

## Integration Examples

### With HTTP API

```go
app.Post("/api/predict", func(c *fiber.Ctx) error {
    var req struct {
        ModelID string      `json:"model_id"`
        Data    interface{} `json:"data"`
    }
    
    if err := c.BodyParser(&req); err != nil {
        return err
    }
    
    output, err := manager.Predict(c.Context(), &ai.InferenceInput{
        ModelID: req.ModelID,
        Data:    req.Data,
    })
    
    if err != nil {
        return c.Status(500).JSON(fiber.Map{"error": err.Error()})
    }
    
    return c.JSON(fiber.Map{
        "result":  output.Result,
        "latency": output.Latency.Milliseconds(),
    })
})
```

### With Background Jobs

```go
// Process ML tasks in background
go func() {
    for task := range taskQueue {
        output, _ := pipelineManager.Execute(ctx, task.PipelineID, task.Input)
        task.Callback(output)
    }
}()
```

## Files

- **model.go** (400+ lines) - Model management and inference
- **cache.go** (200+ lines) - Inference result caching
- **provider_openai.go** (300+ lines) - OpenAI API integration
- **feature_store.go** (350+ lines) - Feature storage and serving
- **pipeline.go** (250+ lines) - ML pipeline orchestration
- **README.md** - Documentation

## Contributing

See main project CONTRIBUTING.md

## License

MIT License
